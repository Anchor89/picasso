{
  "name": "Picasso: Unleash The Power of Nonconvex Penalty",
  "tagline": "From LASSO to SCAD and beyond ",
  "body": "# PICASSO\r\n## Installation in R\r\n\r\n```R\r\n> library(devtools)\r\n> devtools::install_github(\"jasonge27/picasso\")\r\n```\r\n\r\n## Unleash the power of nonconvex penalty\r\n\r\nL1 penalized linear regression (LASSO) is great for feature selection in linear regression. However when you use LASSO in very noisy setting, especially when some columns in your data has strong colinearity, it's easy to see that LASSO tends to give biased estimator due to the penalty term. As demonstrated in the example below, the lowest estimation error among all the lambdas computed is as high as **10.589%**.\r\n\r\n```R\r\n> set.seed(2016)\r\n> library(glmnet)\r\n> n <- 2000; p <- 1000; c <- 0.1\r\n> # n sample number, p dimension, c correlation parameter\r\n> X <- scale(matrix(rnorm(n*p),n,p)+c*rnorm(n)) # n is smaple number, \r\n> s <- 20  # sparsity level\r\n> true_beta <- c(runif(s), rep(0, p-s))\r\n> Y <- X%*%true_beta + rnorm(n)\r\n> fitg<-glmnet(X,Y,family=\"gaussian\")\r\n> # the minimal estimation error |\\hat{beta}-beta| / |beta|\r\n> min(apply(abs(fitg$beta - true_beta), MARGIN=2, FUN=sum))/sum(abs(true_beta))\r\n[1] 0.10589\r\n```\r\n\r\n\r\nNonconvex penalties such as SCAD [1] and MCP [2] are statistically better but computationally harder. The solution for SCAD/MCP penalized linear model has much less estimation error than lasso but calculating the estimator involves non-convex optimization. With limited computation resource, we can only get a local optimum which probably lacks the good property of the global optimum. \r\n\r\nThe PICASSO package [3, 4]  solves non-convex optimization through multi-stage convex relaxation. Although we only find a local minimum, it can be proved that this local minimum does not lose the superior statistcal property of the global minimum. Multi-stage convex relaxation is also much more stable than other packages (see benchmark below). \r\n\r\nLet's see PICASSO in action â€” the estimation error drops to **3.4%** using SCAD penalty from **10.57%** error produced by LASSO.\r\n\r\n```R\r\n> library(picasso)\r\n> fitp <- picasso(X, Y, family=\"gaussian\", method=\"scad\")\r\n> min(apply(abs(fitp$beta-true_beta), MARGIN=2, FUN=sum))/sum(abs(true_beta))\r\n[1] 0.03392717\r\n```\r\n\r\n\r\n\r\n## Fast and Stable\r\n\r\nAs a traditional LASSO solver, our package achieves state-of-the-art performance. For SCAD regularzied linear/logistic regression, it can be shown that our package is much faster and more stable than the alternative ncvreg. (The experiments can also be done with MCP penalty.)  More experiments can be found in vignettes/PICASSO.pdf\r\n\r\nFor the experiments, sample number is denoted by n and sample dimension is denoted by p. The parameter c is used to control the column-wise correlation of the desgin matrix X as before.\r\n\r\n### State-of-the-art LASSO Solver\r\n\r\nWe're as fast as glmnet for L1 regularized linear/logistic regression. Here we benchmark logistic regression as an example. Parameter c is used to add correlation between columns of X to mimic multi-colinearity. PICASSO is the best solver when we have multi-colinearity in the data.\r\n\r\n```R\r\nsource(\"tests/test_picasso.R\")\r\ntest_lognet(n=2000, p=1000, c=0.1)\r\ntest_lognet(n=2000, p=1000, c=0.5)\r\ntest_lognet(n=2000, p=1000, c=1.0)\r\n```\r\n\r\n|         |     c=0.1     |    c=0.5     |    c= 1.0    |\r\n| :-----: | :-----------: | :----------: | :----------: |\r\n| PICASSO |   1.526(s)    |   0.721(s)   |   0.718(s)   |\r\n| glmnet  |   1.494(s)    |   0.845(s)   |   1.743(s)   |\r\n| ncvreg  | 10.564(s) [*] | 7.825(s) [#] | 5.458(s) [#] |\r\n\r\n'[*]': Package exited with warning: Algorithm failed to converge for some values of lambda.\r\n\r\n'[#]': Package exited with error messages.\r\n\r\n### Nonconvex Penalty\r\n\r\nAs glmnet does not provide nonconvex penalty solver, we will compare with ncvreg for run-time and best estimation error along the regularization path.\r\n\r\nFor well-conditioned cases when there's no multi-colinearity, LASSO tends to have lower estimation error. However, as c becomes larger, LASSO's estimation error quickly increases. Nonconvex penalty can be very helpful when some columns of the data are highly correlated. \r\n\r\n```R\r\nsource('tests/test_picasso.R')\r\ntest_lognet_nonlinear(n=3000, p =3000, c=0.1)\r\ntest_lognet_nonlinear(n=3000, p =3000, c=0.5)\r\ntest_lognet_nonlinear(n=3000, p =3000, c=1.0)\r\n```\r\n\r\nTiming for SCAD regularized logistic regression. Estimation error is calculated by finding the best approximation to the true regression coefficient across all regularization parameter.\r\n\r\n```R\r\nmin(apply(abs(fitted.model$beta - true_beta), MARGIN=2, FUN=sum))/sum(abs(true_beta))\r\n```\r\n\r\n\r\n\r\n|                                        |     c = 0.1     |     c = 0.5     |       c = 1.0        |\r\n| :------------------------------------: | :-------------: | :-------------: | :------------------: |\r\n|          PICASSO (time/error)          | 8.727(s) / 4.6% | 5.247(s) / 2.6% |   6.062(s) / 11.2%   |\r\n|          ncvreg (time/error)           | 7.461(s) / 5.6% | 7.056(s) / 6.0% | 51.85(s) / 35.3% [*] |\r\n| Estimation Error using LASSO in glmnet |      2.0%       |      14.1%      |        28.7%         |\r\n\r\n'[*]': Package exited with warning: Algorithm failed to converge for some values of lambda.\r\n\r\nThe experiments are conducted on a MacBook Pro with 2.4GHz Intel Core i5 and 8GB RAM. R version is 3.3.0. The ncvreg version is 3.5-2. The glmnet version is 2.0-5.\r\n\r\n\r\n\r\nReferences\r\n\r\n[1] Jianqing Fan and Runze Li, Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties, 2001\r\n\r\n[2] Cun-Hui Zhang, Nearly Unbiased Variable Selection Under Minimax Concave Penalty, 2010\r\n\r\n[3] Jason Ge, Mingyi Hong, Mengdi Wang, Han Liu, and Tuo Zhao, Homotopy Active Set Proximal Newton Algorithm for Sparse Learning, 2016\r\n\r\n[4] Tuo Zhao, Han Liu, and Tong Zhang, Pathwise Coordinate Optimization for Nonconvex Sparse Learning: Algorithm and Theory, 2014\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}
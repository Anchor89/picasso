<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Picasso: Unleash The Power of Nonconvex Penalty by jasonge27</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Picasso: Unleash The Power of Nonconvex Penalty</h1>
      <h2 class="project-tagline">From LASSO to SCAD and beyond </h2>
      <a href="https://github.com/jasonge27/picasso" class="btn">View on GitHub</a>
      <a href="https://github.com/jasonge27/picasso/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/jasonge27/picasso/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="picasso" class="anchor" href="#picasso" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PICASSO</h1>

<h2>
<a id="installation-in-r" class="anchor" href="#installation-in-r" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation in R</h2>

<div class="highlight highlight-source-r"><pre><span class="pl-k">&gt;</span> library(<span class="pl-smi">devtools</span>)
<span class="pl-k">&gt;</span> <span class="pl-e">devtools</span><span class="pl-k">::</span>install_github(<span class="pl-s"><span class="pl-pds">"</span>jasonge27/picasso<span class="pl-pds">"</span></span>)</pre></div>

<h2>
<a id="unleash-the-power-of-nonconvex-penalty" class="anchor" href="#unleash-the-power-of-nonconvex-penalty" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Unleash the power of nonconvex penalty</h2>

<p>L1 penalized regression (LASSO) is great for feature selection. However when you use LASSO in very noisy setting, especially when some columns in your data have strong colinearity, LASSO tends to give biased estimator due to the penalty term. As demonstrated in the example below, the lowest estimation error among all the lambdas computed is as high as <strong>10.589%</strong>.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-k">&gt;</span> set.seed(<span class="pl-c1">2016</span>)
<span class="pl-k">&gt;</span> library(<span class="pl-smi">glmnet</span>)
<span class="pl-k">&gt;</span> <span class="pl-smi">n</span> <span class="pl-k">&lt;-</span> <span class="pl-c1">2000</span>; <span class="pl-smi">p</span> <span class="pl-k">&lt;-</span> <span class="pl-c1">1000</span>; <span class="pl-smi">c</span> <span class="pl-k">&lt;-</span> <span class="pl-c1">0.1</span>
<span class="pl-k">&gt;</span> <span class="pl-c"># n sample number, p dimension, c correlation parameter</span>
<span class="pl-k">&gt;</span> <span class="pl-smi">X</span> <span class="pl-k">&lt;-</span> scale(<span class="pl-k">matrix</span>(rnorm(<span class="pl-smi">n</span><span class="pl-k">*</span><span class="pl-smi">p</span>),<span class="pl-smi">n</span>,<span class="pl-smi">p</span>)<span class="pl-k">+</span><span class="pl-smi">c</span><span class="pl-k">*</span>rnorm(<span class="pl-smi">n</span>)) <span class="pl-c"># n is smaple number, </span>
<span class="pl-k">&gt;</span> <span class="pl-smi">s</span> <span class="pl-k">&lt;-</span> <span class="pl-c1">20</span>  <span class="pl-c"># sparsity level</span>
<span class="pl-k">&gt;</span> <span class="pl-smi">true_beta</span> <span class="pl-k">&lt;-</span> c(runif(<span class="pl-smi">s</span>), rep(<span class="pl-c1">0</span>, <span class="pl-smi">p</span><span class="pl-k">-</span><span class="pl-smi">s</span>))
<span class="pl-k">&gt;</span> <span class="pl-smi">Y</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">X</span><span class="pl-k">%*%</span><span class="pl-smi">true_beta</span> <span class="pl-k">+</span> rnorm(<span class="pl-smi">n</span>)
<span class="pl-k">&gt;</span> <span class="pl-smi">fitg</span><span class="pl-k">&lt;-</span>glmnet(<span class="pl-smi">X</span>,<span class="pl-smi">Y</span>,<span class="pl-v">family</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>gaussian<span class="pl-pds">"</span></span>)
<span class="pl-k">&gt;</span> <span class="pl-c"># the minimal estimation error |\hat{beta}-beta| / |beta|</span>
<span class="pl-k">&gt;</span> min(apply(abs(<span class="pl-smi">fitg</span><span class="pl-k">$</span><span class="pl-smi">beta</span> <span class="pl-k">-</span> <span class="pl-smi">true_beta</span>), <span class="pl-v">MARGIN</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">FUN</span><span class="pl-k">=</span><span class="pl-smi">sum</span>))<span class="pl-k">/</span>sum(abs(<span class="pl-smi">true_beta</span>))
[<span class="pl-c1">1</span>] <span class="pl-c1">0.10589</span></pre></div>

<p>Nonconvex penalties such as SCAD [1] and MCP [2] are statistically better but computationally harder. The solution for SCAD/MCP penalized linear model has much less estimation error than lasso but calculating the estimator involves non-convex optimization. With limited computation resource, we can only get a local optimum which probably lacks the good property of the global optimum. </p>

<p>The PICASSO package [3, 4]  solves non-convex optimization through multi-stage convex relaxation. Although we only find a local minimum, it can be proved that this local minimum does not lose the superior statistcal property of the global minimum. Multi-stage convex relaxation is also much more stable than other packages (see benchmark below). </p>

<p>Let's see PICASSO in action â€” the estimation error drops to <strong>3.4%</strong> using SCAD penalty from <strong>10.57%</strong> error produced by LASSO.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-k">&gt;</span> library(<span class="pl-smi">picasso</span>)
<span class="pl-k">&gt;</span> <span class="pl-smi">fitp</span> <span class="pl-k">&lt;-</span> picasso(<span class="pl-smi">X</span>, <span class="pl-smi">Y</span>, <span class="pl-v">family</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>gaussian<span class="pl-pds">"</span></span>, <span class="pl-v">method</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>scad<span class="pl-pds">"</span></span>)
<span class="pl-k">&gt;</span> min(apply(abs(<span class="pl-smi">fitp</span><span class="pl-k">$</span><span class="pl-smi">beta</span><span class="pl-k">-</span><span class="pl-smi">true_beta</span>), <span class="pl-v">MARGIN</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">FUN</span><span class="pl-k">=</span><span class="pl-smi">sum</span>))<span class="pl-k">/</span>sum(abs(<span class="pl-smi">true_beta</span>))
[<span class="pl-c1">1</span>] <span class="pl-c1">0.03392717</span></pre></div>

<h2>
<a id="fast-and-stable" class="anchor" href="#fast-and-stable" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fast and Stable</h2>

<p>As a traditional LASSO solver, our package achieves state-of-the-art performance. For SCAD regularzied linear/logistic regression, it can be shown that our package is much faster and more stable than the alternative ncvreg. (The experiments can also be done with MCP penalty.)  More experiments can be found in vignettes/PICASSO.pdf</p>

<p>For the experiments, sample number is denoted by n and sample dimension is denoted by p. The parameter c is used to control the column-wise correlation of the desgin matrix X as before.</p>

<h3>
<a id="state-of-the-art-lasso-solver" class="anchor" href="#state-of-the-art-lasso-solver" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>State-of-the-art LASSO Solver</h3>

<p>We're as fast as glmnet for L1 regularized linear/logistic regression. Here we benchmark logistic regression as an example. Parameter c is used to add correlation between columns of X to mimic multi-colinearity. PICASSO is the best solver when we have multi-colinearity in the data.</p>

<div class="highlight highlight-source-r"><pre>source(<span class="pl-s"><span class="pl-pds">"</span>tests/test_picasso.R<span class="pl-pds">"</span></span>)
test_lognet(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">2000</span>, <span class="pl-v">p</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">0.1</span>)
test_lognet(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">2000</span>, <span class="pl-v">p</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">0.5</span>)
test_lognet(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">2000</span>, <span class="pl-v">p</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">1.0</span>)</pre></div>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">c=0.1</th>
<th align="center">c=0.5</th>
<th align="center">c= 1.0</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">PICASSO</td>
<td align="center">1.526(s)</td>
<td align="center">0.721(s)</td>
<td align="center">0.718(s)</td>
</tr>
<tr>
<td align="center">glmnet</td>
<td align="center">1.494(s)</td>
<td align="center">0.845(s)</td>
<td align="center">1.743(s)</td>
</tr>
<tr>
<td align="center">ncvreg</td>
<td align="center">10.564(s) [*]</td>
<td align="center">7.825(s) [#]</td>
<td align="center">5.458(s) [#]</td>
</tr>
</tbody>
</table>

<p>'[*]': Package exited with warning: Algorithm failed to converge for some values of lambda.</p>

<p>'[#]': Package exited with error messages.</p>

<h3>
<a id="nonconvex-penalty" class="anchor" href="#nonconvex-penalty" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nonconvex Penalty</h3>

<p>As glmnet does not provide nonconvex penalty solver, we will compare with ncvreg for run-time and best estimation error along the regularization path.</p>

<p>For well-conditioned cases when there's no multi-colinearity, LASSO tends to have lower estimation error. However, as c becomes larger, LASSO's estimation error quickly increases. Nonconvex penalty can be very helpful when some columns of the data are highly correlated. </p>

<div class="highlight highlight-source-r"><pre>source(<span class="pl-s"><span class="pl-pds">'</span>tests/test_picasso.R<span class="pl-pds">'</span></span>)
test_lognet_nonlinear(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">p</span> <span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">0.1</span>)
test_lognet_nonlinear(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">p</span> <span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">0.5</span>)
test_lognet_nonlinear(<span class="pl-v">n</span><span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">p</span> <span class="pl-k">=</span><span class="pl-c1">3000</span>, <span class="pl-v">c</span><span class="pl-k">=</span><span class="pl-c1">1.0</span>)</pre></div>

<p>Timing for SCAD regularized logistic regression. Estimation error is calculated by finding the best approximation to the true regression coefficient across all regularization parameter.</p>

<div class="highlight highlight-source-r"><pre>min(apply(abs(<span class="pl-smi">fitted.model</span><span class="pl-k">$</span><span class="pl-smi">beta</span> <span class="pl-k">-</span> <span class="pl-smi">true_beta</span>), <span class="pl-v">MARGIN</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">FUN</span><span class="pl-k">=</span><span class="pl-smi">sum</span>))<span class="pl-k">/</span>sum(abs(<span class="pl-smi">true_beta</span>))</pre></div>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">c = 0.1</th>
<th align="center">c = 0.5</th>
<th align="center">c = 1.0</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">PICASSO (time/error)</td>
<td align="center">8.727(s) / 4.6%</td>
<td align="center">5.247(s) / 2.6%</td>
<td align="center">6.062(s) / 11.2%</td>
</tr>
<tr>
<td align="center">ncvreg (time/error)</td>
<td align="center">7.461(s) / 5.6%</td>
<td align="center">7.056(s) / 6.0%</td>
<td align="center">51.85(s) / 35.3% [*]</td>
</tr>
<tr>
<td align="center">Estimation Error using LASSO in glmnet</td>
<td align="center">2.0%</td>
<td align="center">14.1%</td>
<td align="center">28.7%</td>
</tr>
</tbody>
</table>

<p>'[*]': Package exited with warning: Algorithm failed to converge for some values of lambda.</p>

<p>The experiments are conducted on a MacBook Pro with 2.4GHz Intel Core i5 and 8GB RAM. R version is 3.3.0. The ncvreg version is 3.5-2. The glmnet version is 2.0-5.</p>

<p>References</p>

<p>[1] Jianqing Fan and Runze Li, Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties, 2001</p>

<p>[2] Cun-Hui Zhang, Nearly Unbiased Variable Selection Under Minimax Concave Penalty, 2010</p>

<p>[3] Jason Ge, Mingyi Hong, Mengdi Wang, Han Liu, and Tuo Zhao, Homotopy Active Set Proximal Newton Algorithm for Sparse Learning, 2016</p>

<p>[4] Tuo Zhao, Han Liu, and Tong Zhang, Pathwise Coordinate Optimization for Nonconvex Sparse Learning: Algorithm and Theory, 2014</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/jasonge27/picasso">Picasso: Unleash The Power of Nonconvex Penalty</a> is maintained by <a href="https://github.com/jasonge27">jasonge27</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
